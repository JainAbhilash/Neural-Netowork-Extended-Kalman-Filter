@article{knuth:1984,
  title={Literate Programming},
  author={Donald E. Knuth},
  journal={The Computer Journal},
  volume={27},
  number={2},
  pages={97--111},
  year={1984},
  publisher={Oxford University Press}
}
@misc{Dua:2019 ,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

@article{Bikes, 
year={2013}, 
issn={2192-6352}, 
journal={Progress in Artificial Intelligence}, 
doi={10.1007/s13748-013-0040-3}, 
title={Event labeling combining ensemble detectors and background knowledge}, 
url={[Web Link]}, 
publisher={Springer Berlin Heidelberg}, 
keywords={Event labeling; Event detection; Ensemble learning; Background knowledge}, 
author={Fanaee-T, Hadi and Gama, Joao}, 
pages={1-15} 
}
@article{Main_EKS_Paper,
title = "A sequential learning method with Kalman filter and extreme learning machine for regression and time series forecasting",
journal = "Neurocomputing",
volume = "337",
pages = "235 - 250",
year = "2019",
issn = "0925-2312",
doi = "https://doi.org/10.1016/j.neucom.2019.01.070",
url = "http://www.sciencedirect.com/science/article/pii/S0925231219300980",
author = "Jarley P. Nóbrega and Adriano L.I. Oliveira",
keywords = "Online sequential learning, Time series forecasting, Online Sequential Extreme Learning Machine, Kalman filter, Multicollinearity",
abstract = "In many regression and time series forecasting problems, the input data is not fully available at the beginning of the training phase. Conventional machine learning methods for batch data are not able to handle this problem. The sequential version of ELM, called Online Sequential Extreme Learning Machine (OS-ELM), addresses this problem through the least squares recursive solution for updating the network output weights. However, the implementation of OS-ELM and its extensions suffer from the problem of multicollinearity and its side effect on the variance of the weight estimates. This paper introduces a new method of sequential learning for handling the effects of multicollinearity. The proposed method, called Kalman Learning Machine (KLM), uses the Kalman filter to sequentially update the output weights of a Single Layer Feedforward Network (SLFN) based on OS-ELM. An extension of the proposed method, called Extended Kalman Learning Machine (EKLM), is presented in order to address the problem of nonlinear data. The proposed method was compared with some of the most recent and effective methods for handling the effects of multicollinearity in sequential learning problems. The experiments performed showed that the proposed method performs better than most state-of-the-art methods considering both the prediction error and training time."
}

@article{Winequality,
title = "Modeling wine preferences by data mining from physicochemical properties",
journal = "Decision Support Systems",
volume = "47",
number = "4",
pages = "547 - 553",
year = "2009",
note = "Smart Business Networks: Concepts and Empirical Evidence",
issn = "0167-9236",
doi = "https://doi.org/10.1016/j.dss.2009.05.016",
url = "http://www.sciencedirect.com/science/article/pii/S0167923609001377",
author = "Paulo Cortez and António Cerdeira and Fernando Almeida and Telmo Matos and José Reis",
keywords = "Sensory preferences, Regression, Variable selection, Model selection, Support vector machines, Neural networks",
abstract = "We propose a data mining approach to predict human wine taste preferences that is based on easily available analytical tests at the certification step. A large dataset (when compared to other studies in this domain) is considered, with white and red vinho verde samples (from Portugal). Three regression techniques were applied, under a computationally efficient procedure that performs simultaneous variable and model selection. The support vector machine achieved promising results, outperforming the multiple regression and neural network methods. Such model is useful to support the oenologist wine tasting evaluations and improve wine production. Furthermore, similar techniques can help in target marketing by modeling consumer tastes from niche markets."
}

@Article{Chernodub2014,
author="Chernodub, A. N.",
title="Training Neural Networks for classification using the Extended Kalman Filter: A comparative study",
journal="Optical Memory and Neural Networks",
year="2014",
month="Apr",
day="01",
volume="23",
number="2",
pages="96--103",
abstract="Feedforward Neural Networks training for classification problem is considered. The Extended Kalman Filter, which has been earlier used mostly for training Recurrent Neural Networks for prediction and control, is suggested as a learning algorithm. Implementation of the cross-entropy error function for mini-batch training is proposed. Popular benchmarks are used to compare the method with the gradient-descent, conjugate-gradients and the BFGS (Broyden-Fletcher-Goldfarb-Shanno) algorithm. The influence of mini-batch size on time and quality of training is investigated. The algorithms under consideration implemented as MATLAB scripts are available for free download.",
issn="1934-7898",
doi="10.3103/S1060992X14020088",
url="https://doi.org/10.3103/S1060992X14020088"
}


@techreport{Welch:1995:IKF:897831,
 author = {Welch, Greg and Bishop, Gary},
 title = {An Introduction to the Kalman Filter},
 year = {1995},
 source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Auncch_cs%3AUNCCH_CS%2F%2FTR95-041},
 publisher = {University of North Carolina at Chapel Hill},
 address = {Chapel Hill, NC, USA},
} 

